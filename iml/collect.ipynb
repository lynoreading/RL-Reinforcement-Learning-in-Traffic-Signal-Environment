{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import pylab\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import shapiro, normaltest\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Local imports\n",
    "from local_models import *\n",
    "from helper_functions import *\n",
    "from piece_hurdle_model import *\n",
    "from optimize_explanations import *\n",
    "from evaluation_metrics import *\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_dataloaders():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.FashionMNIST(\n",
    "        root='./data/after_anon_review',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_set = torchvision.datasets.FashionMNIST(\n",
    "        root='./data/after_anon_review',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:00<00:00, 27398872.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/after_anon_review\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 1593672.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/after_anon_review\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 17149572.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/after_anon_review\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/after_anon_review\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/after_anon_review\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "G, cnn = load_models(CNN, Generator)\n",
    "\n",
    "#train_loader, test_loader = load_dataloaders()\n",
    "train_loader, test_loader = load_fashion_dataloaders()\n",
    "#X_train, y_train, X_test, y_test = get_MNIST_data(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST_data(datasets):\n",
    "\tmnist_trainset = datasets.MNIST(root='./data/mnist_train', train=True, download=True, transform=None)\n",
    "\tmnist_testset = datasets.MNIST(root ='./data/mnist_test', train=False, download=True, transform=None)\n",
    "\tX_train = mnist_trainset.data\n",
    "\ty_train = mnist_trainset.targets\n",
    "\tX_test = mnist_testset.data\n",
    "\ty_test = mnist_testset.targets\n",
    "\treturn X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_feature_contribution_data(data_loader, cnn, num_classes=10):\n",
    "    \n",
    "    full_data = dict()\n",
    "    pred_idx = dict() \n",
    "\n",
    "    for class_name in list(range(num_classes)):\n",
    "        pred_idx[class_name] = list()\n",
    "        \n",
    "    for i, data in enumerate(data_loader):\n",
    "        # print progress\n",
    "        if i % 10000 == 0:\n",
    "            print(  100 * round(i / len(data_loader), 2), \"% complete...\"  )     \n",
    "        image, label = data\n",
    "        label = int(label.detach().numpy())\n",
    "        acts = cnn(image)[1][0].detach().numpy()\n",
    "        pred = int(torch.argmax(  cnn(image)[0]  ).detach().numpy()) \n",
    "        pred_idx[pred].append(acts.tolist())\n",
    "                \n",
    "    return pred_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13478\\AppData\\Local\\Temp\\ipykernel_26924\\2353536552.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  label = int(label.detach().numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % complete...\n",
      "17.0 % complete...\n",
      "33.0 % complete...\n",
      "50.0 % complete...\n",
      "67.0 % complete...\n",
      "83.0 % complete...\n"
     ]
    }
   ],
   "source": [
    "collected_data = return_feature_contribution_data(train_loader, cnn)\n",
    "dist_data = {}\n",
    "\n",
    "# 假设 num_classes 是类别的数量\n",
    "num_classes = 10\n",
    "\n",
    "# 为每个类别创建一个空列表\n",
    "for class_name in range(num_classes):\n",
    "    dist_data[class_name] = {'activations': []}\n",
    "\n",
    "# 将 pred_idx_train 中的数据填充到 dist_data 中\n",
    "for class_name, activations_list in collected_data.items():\n",
    "    # 将 activations_list 转换为 numpy 数组\n",
    "    activations_array = np.array(activations_list)\n",
    "    # 将 activations_array 存储到 dist_data 对应的类别中\n",
    "    dist_data[class_name]['activations'] = activations_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功存储为 pickle 文件。\n"
     ]
    }
   ],
   "source": [
    "with open('collected_fashion.pickle', 'wb') as handle:\n",
    "    pickle.dump(dist_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"数据已成功存储为 pickle 文件。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
